<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://xiwan.github.io</id>
    <title>Keep Thinking</title>
    <updated>2020-04-24T10:11:58.592Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://xiwan.github.io"/>
    <link rel="self" href="https://xiwan.github.io/atom.xml"/>
    <subtitle>有美人兮心不怿</subtitle>
    <logo>https://xiwan.github.io/images/avatar.png</logo>
    <icon>https://xiwan.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Keep Thinking</rights>
    <entry>
        <title type="html"><![CDATA[族群架构进化论]]></title>
        <id>https://xiwan.github.io/post/fu_2s9jJ4</id>
        <link href="https://xiwan.github.io/post/fu_2s9jJ4">
        </link>
        <updated>2020-04-24T08:09:34.000Z</updated>
        <summary type="html"><![CDATA[<p>最近再调整游戏服务器的架构，有了些感悟，趁着热乎先记录下来。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近再调整游戏服务器的架构，有了些感悟，趁着热乎先记录下来。</p>
<!-- more -->
<h2 id="目前架构">目前架构</h2>
<p>首先祭出一张目前的简单架构图：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1587718706319.png" alt=""></figure>
<p>基本流程是：<br>
1. 用户先连接CDN，获取可用服务器列表<br>
2. 基于算法随机到其中一台游戏服务器（长连接）<br>
3. 游戏服务器内部交互都通过Redis的订阅模式完成。<br>
4. 保留一个Hub服务器，处理全局任务（排行榜之类）</p>
<p>分析下这个结构的好处：<br>
1. 扩展简单，对于游戏压力来说，只要CDN不跨掉就可以了。<br>
2. 跨服经过redis，这个东西比较成熟，使用起来也很简单<br>
3. 业务划分简单，玩家部分走GS, 系统部分走Hub.</p>
<p>当然也有一定的问题：<br>
1. Hub有可能提前成为瓶颈<br>
2. Redis本身无法保存消息</p>
<h2 id="优化版本">优化版本</h2>
<p>这里我准备用一个专门的socket服务器来替代redis，不关注任何业务，只是做简单的转发。这就是前面提到的调节服务器(Intermediary)。它会支持两种转发策略：<br>
1. 点到点：业务对状态敏感，直接转发。<br>
2. 随机点：业务对状态并不敏感，随机转发。</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1587720969390.png" alt=""></figure>
<p>这里虚线框中的服务器可以按照业务组成一个<strong>族群</strong>，那么这个<strong>族群的首领</strong>就是调节服务器。它扮演的角色就是：<br>
1. 统一收发对外消息<br>
2. 内部协调消息扭转<br>
3. 消息状态的维护</p>
<p>由于Hub变成了无状态的，所以它同样和GS一样十分方便的可以扩展。</p>
<h2 id="扩展版本">扩展版本</h2>
<p>其实我们可以把整个服务器架构都按照这种<strong>族群</strong>的思路来设计。其中调节服务器就类似于“网关”。他们是进出整个<strong>族群</strong>的出入口。那么我们可以看到一个最简单的服务器就是一个<strong>族群</strong>。当我们需要它按照业务无限扩容时候，就变成如下的一个拓扑结构：</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1587721806494.png" alt=""></figure>
<p>这里的业务也是多种定义：可以是普通的<strong>游戏逻辑族群</strong>，也可以是专门的<strong>战斗族群</strong>，或者是<strong>任务族群</strong>，甚至是<strong>存储族群</strong>都可以的。</p>
<p>可以看到这套架构是比较松耦合的，大家都是靠通信协议在<strong>协调服务器</strong>通信。新的业务加入和去除也比较方便。</p>
<p>可能这里对于<strong>协调服务器</strong>的单点情况是有点担心的。我这里考虑到的做法：<br>
1. 利用虚地址或者一致性hash之类保证它的高可用（横向）<br>
2. 提升本身机器的性能（纵向）<br>
3. 根据业务动态的裂变或者合并这个节点</p>
<p>前面两点比较容易理解。第三点的意思是，比如我很多业务都走Hub服务器族群，让它的协调服务器变得压力很大，这时我们可以考虑拆分这些业务：比如把排行榜和定时任务拆开。这样就变成了两个族群。同理，反向操作也是可以的。</p>
<p>关系图如下：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1587722838681.png" alt=""></figure>
<h2 id="总结">总结</h2>
<p>可以看到整个架构的一个进化过程其实是很自然也很动态的。但需要配合很多CI/CD的工具来达到这个目的。不过后面我会先用netmq来实现这个框架。</p>
<h3 id="参考">参考</h3>
<p>https://netmq.readthedocs.io/en/latest/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[用一首歌的时间落地10万个用户数据]]></title>
        <id>https://xiwan.github.io/post/21M9ANIq1</id>
        <link href="https://xiwan.github.io/post/21M9ANIq1">
        </link>
        <updated>2020-03-25T07:47:42.000Z</updated>
        <summary type="html"><![CDATA[<p>为啥是一首歌，因为新的需求是极端情况可以忍受玩家回档5分钟。哪些极端情况呢？redis完全不能用了，只能走mysql来落地。来保护玩家数据。</p>
<p>不过有个明显的问题是：mysql到底能不能抗此大责任？</p>
]]></summary>
        <content type="html"><![CDATA[<p>为啥是一首歌，因为新的需求是极端情况可以忍受玩家回档5分钟。哪些极端情况呢？redis完全不能用了，只能走mysql来落地。来保护玩家数据。</p>
<p>不过有个明显的问题是：mysql到底能不能抗此大责任？</p>
<!-- more -->
<h3 id="介绍">介绍</h3>
<p>首先我们得知道每个用户大概占多少内存。根据线上用户的统计来看，差不多可以认为上限为200kb</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1585122972394.png" alt=""></figure>
<p>如果我们要保存10万个用户，差不多就是100,000x200Kb = 20G。这个对redis来说已经算比较大的压力了。所以自然考虑到了mysql。</p>
<p>mysql在这里主要扮演的是备份和还原的角色</p>
<ul>
<li>备份：定时把内存中的用户信息写入mysql，对时间比较敏感，同时需要保证数据的完整性。</li>
<li>还原：回滚用户数据时候，停服将mysql里面数据加载到redis，对时间不是特别敏感。</li>
</ul>
<h3 id="做法">做法</h3>
<p>比较常规的做法是用一个循环套住insert或者update语句，如果10万个用户差不多需要跑10次。比较直接，但反复操作数据库连接太消耗，故没有考虑。</p>
<p>查询资料后，发现还有四种技巧：</p>
<ol>
<li>insert into ...on duplicate key update批量更新</li>
<li>replace into 批量更新</li>
<li>创建临时表，先更新临时表，然后从临时表中update</li>
<li>使用mysql 自带的语句构建批量更新</li>
</ol>
<p>进过研究，比较适合我这个项目的是方法2. 1和4主要是构建的sql语法比较麻烦，并且将业务逻辑下沉到数据库本身不是一种提倡的做法。3的开销主要是建立表，如果中间出现数据库问题，也是代价比较大。</p>
<h3 id="迭代">迭代</h3>
<p>由于内存限制，不大可能把10万个用户都加载到内存。所以准备加载1000个用户，然后循环100次。</p>
<p>第一个版本比较直接，就是将内存中所有用户全部构建成sql语句来执行。测试速度如下：</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1585124385650.png" alt=""></figure>
<p>可以看到似乎不错，差不多10分钟内可以完成整个过程。但是离我们的目标5分钟还是有距离。</p>
<p>第二个版本：我固定了每次执行的用户数目，比如500个。这样改造后的速度：</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1585124674444.png" alt=""></figure>
<p>已下子到了6分钟。看来批量定长是一个不错的方向。</p>
<p>第三个版本：由于数据本身200kb一个有压缩空间，所以我把每个用户数据进行了gzip压缩再存储。这次到达速度为：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1585124866510.png" alt=""></figure>
<p>结果不错，已经到达5分钟级别了。那么还能不能更快呢？我做了第四个版本是多线程的，但结果不是特别好：</p>
<figure data-type="image" tabindex="5"><img src="https://xiwan.github.io/post-images/1585125335885.png" alt=""></figure>
<p>这个版本基本和第三版本持平，唯一的好处做成一个异步的过程。当然也有可能是我实现得不对:)</p>
<h3 id="如何进一步挖掘潜力">如何进一步挖掘潜力</h3>
<p>似乎单机10万写入速度已经到了极限，提升不大了。这个时候比较好的思路是拆表了，加入我把10万个用户拆成5个表，每个表只负责2万人。相信整体速度会有个更大的提升。</p>
<p>由于游戏的规模可能就是10万人同时在线，所以每台服务器内存很可能就是只负责差不多这10万人的一部分。那么为什么不直接用一张表来放呢？</p>
<p>其实这里基于两点考虑：<br>
1.多个线程写一张表可能会出现竞争甚至死锁的问题。<br>
2.然后玩家可能会在多个服务器都有记录，这个时候由于同步机制的问题，很可能老的数据反而覆盖了新的数据问题。</p>
<p>当然分成多个表带来的问题是如何识别重复数据哪份更新。最简单的做法是使用时间戳， 不过考虑到机器的时间差，其实用时间戳不是一个特别好的方案。还是需要使用redis或者zookeeper加锁机制来获取一个递增的版本号来管理。当发现两份数据的key值相同时候，通过对比版本号大小即可。</p>
<h3 id="结语">结语</h3>
<p>这次是基于C#的Dapper库做的实验，其他的链接库应该差不多。主要是这一通操作的思想：尽量把相同的数据批处理，然后是定长，再配合压缩和异步的技术，那么这么大的数据也是可以写入mysql的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一种防止客户端作弊的方案]]></title>
        <id>https://xiwan.github.io/post/l4Y3atzIB</id>
        <link href="https://xiwan.github.io/post/l4Y3atzIB">
        </link>
        <updated>2020-03-02T09:57:39.000Z</updated>
        <summary type="html"><![CDATA[<p>个人觉得要防住外挂，最好的方案就是一切走服务器来验证。尤其是一些关键性的数据，比如血条，装备和位置。</p>
]]></summary>
        <content type="html"><![CDATA[<p>个人觉得要防住外挂，最好的方案就是一切走服务器来验证。尤其是一些关键性的数据，比如血条，装备和位置。</p>
<!-- more -->
<p>这里我以位置来举例，如果什么时候都要把数据同步给服务器来验证后再移动，除了可能的丢包之外，来回的开销也会让客户端玩起来像PPT一样。当然回合制的游戏，比如卡牌之类可以考虑这种方案。如果是对实时性要求比较高的FPS游戏，肯能就无法忍受这种方案了。</p>
<h3 id="模型思路">模型思路</h3>
<p>有一种比较实用的方案是：由于“位置”这个信息比较敏感，客户端会不停地反复更新它。肯定是需要服务器校验的。所以可以去掉客户端的直接写它的权利，转而做一个“位置副本”。客户端对于位置的更新都给予这个“位置副本”，包括读和写。这样由于操作都在客户端所以游戏的流畅感会好很多。</p>
<p>对于原来的“位置”数据，客户端只保留了读的权利。相反，服务器将拥有“位置”数据的读和写权利，但对于“位置副本”却只有读的权利。</p>
<p>那么当客户端不断的操作“位置副本”，并且不停地将信息同步给服务器的时候。服务器可以拿“位置”数据和它进行交叉对比。如果发现符合<strong>算法条件</strong>的下移动，则可以拿“位置副本”更新“位置”数据。相反，如果发现异常：比如一下子穿墙之类，服务器就不会更新“位置”数据。并且异步告知客户端需要修正它的“位置副本”。</p>
<p>通常客户端是只要关注“位置副本”数据的，一旦收到服务器的“位置”异常信息。则需要立马修正自己的“位置副本”到上一个正确的“位置”数据。这样就达到了一个既能保证流畅性又能完成校验的目的了。</p>
<h3 id="服务器预估">服务器预估</h3>
<p>那么服务器如何判定客户端的“位置副本”正确性呢？</p>
<p>首先， 服务器需要一份地图数据（这份地图数据有可能也需要实时更新，比如在一些可以破坏地形的场景中）。同时他需要保存一份所有玩家的“位置”信息。这样服务器就知道玩家某个时刻在地图的哪个位置了。当它收到客户端的“位置副本”数据更新信息后，除了做一些常规的校验外。最终要的是去匹配当前值是否和“预估位置”值。如果符合，则可以更新。</p>
<p>那么这个“预估位置”如何得到呢？这里会涉及比较多的因素： 比如玩家的三维值，环境值之类。基本上的思路是基于上一个“位置”数据，然后穷尽游戏内所有可以对移动产生的作用的因素，并把它转化成许多个对应的“向量”，然后进行相加就可以获得“预估位置”了。</p>
<p>当然基于现实情况的复杂性，要求“位置副本”和“预估位置”严格相等是显然不现实的。所以还要容忍一定的“合理误差”。</p>
<h3 id="客户端呢">客户端呢？</h3>
<p>由于我不是客户端出生，所以思路有可能比较局限。现在想到的方案有，进行包混淆可以一定层度的减少被破解。然后一些敏感的“位置”信息相关的逻辑可以欺骗性的用字符串而不是常规的数字，这样就可以避免被内存定位到之类。当然肯定不止这些方案。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[调查redis的protocal error]]></title>
        <id>https://xiwan.github.io/post/O3_DlNeqo</id>
        <link href="https://xiwan.github.io/post/O3_DlNeqo">
        </link>
        <updated>2020-02-18T10:43:42.000Z</updated>
        <summary type="html"><![CDATA[<p>去年年底封测的第一个晚上，晚高峰时候，本来一切平和。但是在毫无征兆的情况下，突然线上redis断了。检查了机房连接没有问题，并且线上10多个实例，只有一台机器的redis断掉了。十分诡异！排查了一个小时没有任何结果，此次问题造成了大概800人回档。对于当时近万人的同时在线，还算损失比较小。</p>
]]></summary>
        <content type="html"><![CDATA[<p>去年年底封测的第一个晚上，晚高峰时候，本来一切平和。但是在毫无征兆的情况下，突然线上redis断了。检查了机房连接没有问题，并且线上10多个实例，只有一台机器的redis断掉了。十分诡异！排查了一个小时没有任何结果，此次问题造成了大概800人回档。对于当时近万人的同时在线，还算损失比较小。</p>
<!-- more -->
<p>这个错误在以前的测试，包括压测过程中都没有出现过。并且steam服务器跑了也将近半年没有问题。但当时简单修复相关业务逻辑后，这个错误在封测期间再也没有问题了。所以又搁置了起来。</p>
<p>不过封测停止后，这个问题突然在steam服务器也多了起来。辛亏的是当时做了一个保护措施（一旦发现redis连接断掉就立马重连）。所以还算安稳度过到现在。</p>
<h3 id="如何稳定复现这个问题">如何稳定复现这个问题？</h3>
<p>现在摆在手上的日志只有failed to write时候一定会断线，于是找到我们使用的这个第三方客户端源码研读起来：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1582108891413.png" alt=""></figure>
<p>这就是抛错的地方，的确是连接问题。但是什么条件会抛呢？由于源码实在复杂，所以这部分并没有继续下去。</p>
<h3 id="继续深挖">继续深挖</h3>
<p>由于最近几乎每天redis都会自动断掉然后重连，于是决定直接monitor现场。这个命令的作用就是看redis当时在处理什么命令。具体可以参考官方文档。结果发现监控不多久就出现了如下错误：</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1582109109573.png" alt=""></figure>
<p>然后同样也是连接断掉了。当然redis自己还是稳定运行的。在网上查了下相关问题，很多人给的答案有点让人捉摸不透：</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1582109233761.png" alt=""></figure>
<p>因为问题是出在客户端这边，为什么要修改redis服务器的一些配置甚至去升级呢？就感觉这个做法有点像<strong>头痛医脚</strong>。</p>
<h3 id="柳暗花明">柳暗花明</h3>
<p>就在一筹莫展时候，我阅读了官网的<a href="https://xiwan.github.io/post/FwWrD9HpO/">协议文章</a></p>
<p>于是有了个推论会不会是底层的byte乱了。比如说我多了个CRLF或者少了个CRLF之类。于是做了些测试，的确验证了我的想法：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1582112369686.png" alt=""></figure>
<p>当出现<strong>protocol error</strong>后我自己写的客户端也被断掉了。虽然这个错误和monitor看到的错误并不一样，但看错误日志可以得出他们都属于<strong>protocol error</strong>这个大类型。</p>
<pre><code>当redis抛出protocol error后会立刻关闭客户端的socket
</code></pre>
<p>上面的是我做出的一个结论。</p>
<h3 id="继续测试">继续测试</h3>
<p>有了上面那个结论，我怎么去做测试数据来验证这点呢？因为协议的部分是底层实现不大可能多一个或者少一个<strong>关键字符</strong>。于是我想着自己做一些数据来验证这点。</p>
<figure data-type="image" tabindex="5"><img src="https://xiwan.github.io/post-images/1582109822145.png" alt=""></figure>
<p>程序逻辑大概是随机组合这些保留字符窜丢给redis，不过截至目前为止，我也没有试验出来断线。看样子就协议本身来说还是比较稳定的，只要底层协议按照官方的来，哪怕你输入了一些特殊字符， redis本身还是能顺利处理的。</p>
<p>到了这里，似乎又陷入了死胡同。。。</p>
<h3 id="回到原点">回到原点</h3>
<p>既然错误是redis抛出来的，于是我回到了它的源码上面。很快就找到了一些端倪了：</p>
<figure data-type="image" tabindex="6"><img src="https://xiwan.github.io/post-images/1582110076823.png" alt=""></figure>
<p>这段代码很明显是处理首字符的时候发现了未定义字符而抛错，进而redis认为连接的字符顺序错误了，从而导致了断线。并且有意思的是这段错误是来自hiredis而非redis服务器本身，另一个有意思的是老版本显然支持的协议字符要比上面的更少。</p>
<p>hiredis是redis作者写的一个客户端，可以推测使用的redis-cli命令应该是基于这个开发的，所以用monitor命令时候才会发现这个错误。看样子是回包时候，发现了数据错误。那么这个错误似乎和我们线上问题无关了。因为Failed to write明显是写的时候错误了。</p>
<h3 id="暂时总结">暂时总结</h3>
<ol>
<li>redis命令由于定义了一套自己的协议，第三方协议需要按照它来和redis交互。</li>
<li>出现protocol error必然会断线，所以一定要做好redis连接的保护。</li>
<li>protocol error错误出现后，要关注后面的内容。有些事hiredis发出来的，有些是redis发出来的。</li>
<li>网上说得更改redis设置可以解决protocal错误明显是误导。都没有搞清楚这个错误是hiredis丢出来。</li>
<li>最后我还是觉得写入数据时候有概率写入<strong>魔鬼字符</strong>导致错误，这个需要后续跟进</li>
</ol>
<p>暂时写这么多吧。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis底层协议]]></title>
        <id>https://xiwan.github.io/post/FwWrD9HpO</id>
        <link href="https://xiwan.github.io/post/FwWrD9HpO">
        </link>
        <updated>2020-02-18T09:34:39.000Z</updated>
        <summary type="html"><![CDATA[<p>最近读到官网的一篇文章 https://redis.io/topics/protocol, 主要是描述了一下redis在通讯协议。忽然觉得豁然开朗：redis作为作为一个内存数据库，其实其本质也是一个服务器而已。监听在6379（默认）接口，然后定义了一套自己的命令方便调用者使用。</p>
<p>其实市面上的主流第三方客户端都会遵守这套协议。只不过实现的语言不一样而已。这里试着分析这套协议，并且写一个简单的“redis客户端”。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近读到官网的一篇文章 https://redis.io/topics/protocol, 主要是描述了一下redis在通讯协议。忽然觉得豁然开朗：redis作为作为一个内存数据库，其实其本质也是一个服务器而已。监听在6379（默认）接口，然后定义了一套自己的命令方便调用者使用。</p>
<p>其实市面上的主流第三方客户端都会遵守这套协议。只不过实现的语言不一样而已。这里试着分析这套协议，并且写一个简单的“redis客户端”。</p>
<!-- more -->
<h3 id="内联协议">内联协议</h3>
<p>既然是个tcp服务器，我们就可以telnet上去，类似下图这样：<br>
<img src="https://xiwan.github.io/post-images/1582019455503.png" alt=""></p>
<p>这些命令看起来和我们平时使用的差不多，不过这些命令被称为“内联命令”，而不是redis真正的通信协议。怎么理解呢？就是这些命令发过去后，redis服务器会先分析你的命令，如果发现是“内联的”，就会自己再解析成标准协议。</p>
<p>对于redis这种高性能服务器来说，花费额外的性能解析这些“内联命令”是有些得不偿失的。这里斗胆坏一下：网上有些第三方的客户端做了这种投机取巧的：）</p>
<h3 id="真正的协议">真正的协议</h3>
<p>虽然分为请求和相应两个部分，其实他们都遵循一个协议：</p>
<ul>
<li>对于<strong>简单字符串</strong>类型，它的第一个byte是&quot;+&quot;</li>
<li>对于<strong>错误</strong>类型，它的第一个byte是&quot;-&quot;</li>
<li>对于<strong>整数</strong>类型， 它的第一个byte是&quot;:&quot;</li>
<li>对于<strong>复杂字符串</strong>类型，它的第一个byte是&quot;$&quot;</li>
<li>对于<strong>数组</strong>类型，它的第一个byte是&quot;*&quot;</li>
<li><strong>结束字符</strong>固定为 &quot;\r\n&quot; (CRLF)。</li>
</ul>
<p>对于发送端：客户端永远使用<strong>数组</strong>类型，里面使用<strong>复杂字符串</strong>类型。<br>
所以举个例子，比如<em>get name</em>, 在底层就变成了</p>
<p><code>*2\r\n$3\r\nget\r\n$4\r\nname\r\n</code></p>
<p>这里做下说明： *2表示整个数组长度为2，然后\r\n都是结束字符，可以认为是redis的一种强制分隔符； $3表示get这个字符长度为3；同理, $4表示name字符长度为4。当然每个表达式中间再用CRLF字符分割即可。</p>
<p>这里我写了个简单的拼接程序：</p>
<pre><code>static readonly string CRLF = &quot;\r\n&quot;;
static string SocketCommand(string[] command) 
{
		var _len = command.Length;
		var _command = new StringBuilder();
		_command.Append($&quot;*{_len}{CRLF}&quot;);

		for (int i = 0; i &lt; _len; i++) 
		{
				_command.Append($&quot;${command[i].Length}{CRLF}{command[i]}{CRLF}&quot;);
		}

		return _command.ToString();
}
</code></pre>
<p>测试结果：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1582021232629.png" alt=""></figure>
<p>可以看到和telnet模式是一样的操作，但底层协议完全不一样。</p>
<h3 id="不同的模式">不同的模式</h3>
<p>除了上面说得这种经典“一问一答”模式之外， redis还支持另外两种操作：</p>
<ol>
<li>管道技术(PIPELINE)： 当你有很多命令需要短时间执行时候，这个技术非常有用。其核心就是将多个命令打包成一个命令，减少来回的网络开销。这里有一点说明，管道和事务是有区别的，虽然他们都是合并命令。管道可以认为这些命令是松散的，中间可以插入其他的命令；而事务是相反的，这些命令是紧耦合的，必须全部执行完毕才能执行后续命令。</li>
<li>订阅模式：当客户端订阅(subscribe)某个频道后，redis将不再需要等待命令了。可以反过来在pub端发送消息，而sub端会自动获取到这些消息。这项技术在应用在我们游戏的聊天室中，还有一些跨服消息（rpc）之类。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random函数罢工了]]></title>
        <id>https://xiwan.github.io/post/POXwU3zqV</id>
        <link href="https://xiwan.github.io/post/POXwU3zqV">
        </link>
        <updated>2020-02-09T12:43:49.000Z</updated>
        <summary type="html"><![CDATA[<p>今天下午，突然线上出现大量高级掉落重复出现。<br>
正常的概率是1%，结果部分玩家出现了很多。导致了群里炸了锅。于是开始分析掉落部分代码：</p>
]]></summary>
        <content type="html"><![CDATA[<p>今天下午，突然线上出现大量高级掉落重复出现。<br>
正常的概率是1%，结果部分玩家出现了很多。导致了群里炸了锅。于是开始分析掉落部分代码：</p>
<!-- more -->
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1581252565576.png" alt=""></figure>
<p>这里做过一次优化，以前是不停的new 一个Random对象。后面测试时候发现效率不高，于是采用了复用的方式。现在似乎这里除了问题，分析下来应该是Random在某种条件下输出了相同值。</p>
<h3 id="问题分析">问题分析</h3>
<p>既然Random出了问题，那么有下面几个问题：<br>
1. 触发Random出问题的条件是什么？<br>
2. Random出问题后会输出什么值呢？<br>
3. 假如是固定值，是有条件固定还是无条件呢？</p>
<h4 id="触发条件">触发条件</h4>
<p>有一种说法是：由于不指定seed情况下，random会默认用时间的tick来当作种子。如果在一个tick内重复过多的调用，则会产生固定值。</p>
<p>不过根据线上情况分析：这个问题持续了半小时。所以这个马上被否决了。</p>
<p>接下来搜索后得到了一个线索：</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1581252881498.png" alt=""></figure>
<p>并且有人提到当random停止工作后，他固定返回0。</p>
<p>多线程环境下，的确使用static的random会产生这样的问题。这样来看应该是这里对于random的优化不当了。不过随之而来的是，固定值真的是0么？</p>
<h4 id="固定值">固定值</h4>
<p>停摆后的random的固定输出值是什么？真的是0。</p>
<p>同理，线上的表现否定了这个结果。因为如果是0，玩家的掉落数目也使用了这个随机数，所以他们应该获取不到这个异常的结果。。。</p>
<p>那么究竟是什么值呢？只能通过模拟多线程来压测了</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1581253148318.png" alt=""></figure>
<p>具体代码如上，在多线程环境下模拟随机过程，推论是：如果正常他应该输出比较均匀的分布，如果异常应该可以发现某个值特别大。</p>
<p>当次数低于100000次，我并没有看到太多异常。当超过这个值后有趣的事情发生了：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1581253310151.png" alt=""></figure>
<figure data-type="image" tabindex="5"><img src="https://xiwan.github.io/post-images/1581253317111.png" alt=""></figure>
<figure data-type="image" tabindex="6"><img src="https://xiwan.github.io/post-images/1581253322350.png" alt=""></figure>
<p>这三次异常的结果我差不多测试了30多遍才出来。也就是说差不多只有10%的概率出现Random罢工。这样的确证实了前面的说的多线程情况下， random输出固定值。可以看到一开始应该是均匀分布的，一旦出现多线程问题，马上罢工。</p>
<h4 id="固定条件">固定条件</h4>
<p>Ranom 罢工后会输出固定值这点确认了。但不是网上大家说的是0，而是取决于我输入的min值。这样就解释了线上的大部分疑惑了。并且继续做测试可以发现，这个固定值一旦产生并不是固定不变，而是每次都等于min值。</p>
<p>测试条件为5-10，11-15两个取间同时开始随机取值，检查分布。结果如下图：</p>
<figure data-type="image" tabindex="7"><img src="https://xiwan.github.io/post-images/1581253614580.png" alt=""></figure>
<p>看到当第一个红框出现罢工后，下一个Random值不是5，而是直接都输出成11了。</p>
<h3 id="如何改进呢">如何改进呢？</h3>
<p>原来是通过new 一个Random达到隔离的目的，但现在看来有两个问题：<br>
1. 效率不高<br>
2. 并没有根本解决多线程下罢工的问题。<br>
网上给的方案是采用锁，这样感觉效率也不高。并不能给这个底层方法带来本质的变化。</p>
<p>现在想到的一个方案类似于concurrentDictionary那样，多粒度控制锁的一个Random池子。利用这个数据结构的并发性，来创造多个可以复用的Random实例。每次需要时候采用RondRobin算法获得一个对象即可。</p>
<p>当然更激进的做法就是不用并发对象，直接做1000个实例，感觉出错的概率会下降不少。<br>
以下就是核心代码部分，每次通过到队列中获取random实例，而非反复构造它。当然在竞争十分激烈的情况下有可能拿不到random，这个时候就做了个保底。使用全局的globalRandom.</p>
<pre><code>        static Random DequeueRand(Random random)
        {
            if (random == null)
            {
                if (GlobalRandomQueue.TryDequeue(out Random rand))
                {
                    random = rand;
                    GlobalRandomQueue.Enqueue(rand);
                }
                else
                {
                    // 保底
                    random = globalRandom;
                }
            }
            return random;
        }
</code></pre>
<p>测试效果：测试10批，每批10次， 每次1百万条，random函数再也不出现“罢工“情况了</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线上redis问题分析记录]]></title>
        <id>https://xiwan.github.io/post/9Wy-WVteG</id>
        <link href="https://xiwan.github.io/post/9Wy-WVteG">
        </link>
        <updated>2020-01-17T10:08:27.000Z</updated>
        <content type="html"><![CDATA[<p>首先祭出一般的复杂操作优化方案<br>
<img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e216cc5b43ea.png" alt=""></p>
<ol>
<li>
<p>历史价格不需要拿出全部数据。看现在这部分大小不大都在1KB左右，但是由于时间长了某些物品的价格数量比较多导致list会比较长。</p>
<figure data-type="image" tabindex="1"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e216d0097915.png" alt=""></figure>
</li>
<li>
<p>部分玩家的事务队列是否可能出现堆积，然后队列过长？</p>
</li>
<li>
<p>寄售队列查了日志访问人数不多，将来可以考虑寄售过期之类，防止过长。</p>
</li>
<li>
<p>可以定一个大概取间让队列尽量控制在5000以内，排查下用队列比较多的地方，比如排行榜之类。</p>
</li>
<li>
<p>LDR:0:1 作为排行榜也是最热的key,需要控制大小和长度。</p>
<figure data-type="image" tabindex="2"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e2177ba65644.png" alt=""></figure>
</li>
<li>
<p>建议有expire的地方，如果觉得会有大量数据的前提下，都加一个随机数.像下面这种循环内设置统一过期时间的做法就有点危险</p>
</li>
</ol>
<figure data-type="image" tabindex="3"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e218b722d15b.png" alt=""></figure>
<ol start="7">
<li>
<p>保持主redis只放和用户相关的重要数据，周围数据都可以放在另外的redis中，需要梳理业务。</p>
</li>
<li>
<p>对于stackexchage的连接做保护</p>
</li>
<li>
<p>每日凌晨发现有集中的内存释放，并且空间还比较大.</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1579255870027.png" alt=""></figure>
</li>
</ol>
<p>分析完业务后发现是排行榜集中时间过期，没有做随机延迟。<br>
排行榜大的原因是没有限制数目导致的，并且还有一个安全隐患就是设置数据和expire是两条语句，有可能导致内存泄露。</p>
<pre><code>public static async Task RenewDailyStar(RaidType type, int relatedID, RaidDailyStar star)
        {
            var oldStar = RedisValue.Null;
            var key = GetDailyStarKey(type, relatedID);

            // 防止集中过期
            var expRandom = TimeSpan.FromDays(offlineExpireDay).Add(TimeSpan.FromSeconds(MathHelper.GetRandom(0, 600))); 
            try
            {
                // fetch last daily star
                var cacheStar = SerializationHelper.Serialize(star);
                // getset is atomic
                oldStar = await RedisServer.Instance().DB(dailyStarDB).StringGetSetAsync(key, cacheStar);
                if (oldStar != RedisValue.Null)
                {
                    // compare with oldstar
                    if (oldStar.Get&lt;RaidDailyStar&gt;().GetFinalScore() &gt; star.GetFinalScore())
                        await RedisServer.Instance().DB(dailyStarDB).StringSetAsync(key, oldStar, expRandom);
                }
            }
            catch (Exception ex)
            {
                LogHelper.WriteErrorLog(string.Format(&quot;{0} | {1}&quot;, star.GetFinalScore(), ex.Message));

                // 滚回操作
                if (oldStar != RedisValue.Null)
                    await RedisServer.Instance().DB(dailyStarDB).StringSetAsync(key, oldStar, expRandom);
            }
        }
</code></pre>
<ol start="10">
<li>这里是个批处理，而不是pipeline，由于取了1000位排行榜玩家数据，所以很可能这边会相当耗时。<br>
<img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e217cfe86470.png" alt=""></li>
</ol>
<p>在最外层的调用地方还有一个for循环加持放大</p>
<p><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e21874521588.png" alt=""><br>
分析腾讯的监控看那个面板可以看到，每个一个小时延迟和读数目有个尖峰。应该就是这个原因导致的。</p>
<figure data-type="image" tabindex="5"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e217e17cc74f.png" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[elk时区问题]]></title>
        <id>https://xiwan.github.io/post/zX5LGZjc5</id>
        <link href="https://xiwan.github.io/post/zX5LGZjc5">
        </link>
        <updated>2020-01-09T03:39:22.000Z</updated>
        <summary type="html"><![CDATA[<p>希望通过这篇文章能解释清楚elk中的时区问题</p>
]]></summary>
        <content type="html"><![CDATA[<p>希望通过这篇文章能解释清楚elk中的时区问题</p>
<!-- more -->
<p>由于kibana包括es默认是使用UTC时间，对于我们这种常年驻扎在东八的社畜来说十分不友好。比如我们日志是按照东八时间记录的，但到了elk中间你会发现@timestamp会莫名奇妙的少了8个小时。这是因为底层用的就是UTC。</p>
<p>于是找到一种方案是在Logstash中对齐@timestamp和日志时间：</p>
<pre><code>  ruby { 
    code =&gt; &quot;event['timestamp'] = LogStash::Timestamp.new(event['@timestamp']+ 8*60*60)&quot; 
  }
  
  ruby {
    code =&gt; &quot;event['@timestamp']= event['timestamp']&quot;
  }
</code></pre>
<p>这样做后的结果呢？</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1578541551939.png" alt=""></figure>
<p>由于默认设置kibana用浏览器时间，所以档这个对齐后，上午11点却只能现实凌晨3点的日志。。。感觉越搞越糟糕了。</p>
<p>仔细想下，我们其实强行同步这两个时间是有点多余的。因为kibana默认是按照@timestamp来交割，并且用的是utc时区来展示。当我们切换设置为browser，它自动将@timestamp转换到对应时区，而不影响日志展示本身。所以一旦你强行对齐这两个时间，反而会导致部分日志无法展示出来了。如果你服务器一开始就设定UTC时间来记录日志，这种对齐是没有任何问题得。但是一旦设定了其他时区，其实最好得做法就是简单的把日志时间和时区信息设置到@timestamp，剩下的就让elk自己转换即可。</p>
<pre><code>  date {
    match =&gt; [&quot;[log_json][log_datetime]&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;]
    target =&gt; &quot;@timestamp&quot;
    timezone =&gt; &quot;+08:00&quot;
  }
</code></pre>
<p>但是要记得在查询语句里面和时间相关的条件都带上timezone就好了。其实这么看来，最优雅的做法还是一开始就把服务器时间设置成UTC,可以少走很多弯路。</p>
<p>这里多说一点，elk用这个@timestamp除了展示用还有什么作用呢？索引分割用。由于我们日志是按天分割，日志文件是按小时记录。由于存在8小时时差，所以索引的一天24小时是跨了日志的两天的。比如索引7号的日志，实际是从7号早上8点到8号早上7点为止。这点明白后，在调整索引时候，最好延迟两天再做，防止对正在写入的索引产生影响。别我是怎么知道的，反正都是血泪史。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[elk的fileds被撑爆了。。。]]></title>
        <id>https://xiwan.github.io/post/WBe1HiDrr</id>
        <link href="https://xiwan.github.io/post/WBe1HiDrr">
        </link>
        <updated>2020-01-06T07:41:25.000Z</updated>
        <summary type="html"><![CDATA[<p>今天调查了一个奇怪的问题，在其他日志都是看起来的情况下，突然发现某个日志在5号凌晨开始后没有被收录了。</p>
]]></summary>
        <content type="html"><![CDATA[<p>今天调查了一个奇怪的问题，在其他日志都是看起来的情况下，突然发现某个日志在5号凌晨开始后没有被收录了。</p>
<!-- more -->
<h3 id="定位问题">定位问题</h3>
<p>通过调查日志文件本身发现是有记录的，但是没有进入elk。由于前面我已经加上了日志counter和服务器id，那么可以排除日志hash碰撞的可能性。于是开始调查logstash，得到的日志结果:</p>
<p><strong>[2020-01-06T05:47:02,707][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[&quot;index&quot;, {:_id=&gt;&quot;f5a3e882bfcb4f155c99db18c07093e2&quot;, :_index=&gt;&quot;kakiraid-info-2020.01.06&quot;, :_type=&gt;&quot;kakiraid_log&quot;, :_routing=&gt;nil}, #<a href="LogStash::Event:0x2ccdcf00">LogStash::Event:0x2ccdcf00</a>], :response=&gt;{&quot;index&quot;=&gt;{&quot;_index&quot;=&gt;&quot;kakiraid-info-2020.01.06&quot;, &quot;_type&quot;=&gt;&quot;kakiraid_log&quot;, &quot;_id&quot;=&gt;&quot;f5a3e882bfcb4f155c99db18c07093e2&quot;, &quot;status&quot;=&gt;400, &quot;error&quot;=&gt;{&quot;type&quot;=&gt;&quot;illegal_argument_exception&quot;, &quot;reason&quot;=&gt;&quot;Limit of total fields [1000] in index [kakiraid-info-2020.01.06] has been exceeded&quot;}}}}</strong></p>
<p>原来是索引中的字段给撑爆了。联系起前几天调整的复杂日志结构问题，是有这个可能性的。因为为了分析开卡日志和减少日志放大倍数，为每个item都动态生成了field。没想到在这里给自己挖坑了。</p>
<p>为了确定是不是这个原因，我们需要统计下当时的情况：</p>
<pre><code>curl -s -XGET http://host:port/index/_mapping? pretty | grep '&quot;type&quot;' | wc -l
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1578300163529.png" alt=""></figure>
<p>可以看到3号的field才741个，4号已经飙到1487个了，5号差不多1573个，6号是1463个。默认的field上线是1000个，怪不得会出现上面的错误，并且可以推断丢失的日志应该不止一种。只能说比较幸运的是，影响范围不大。</p>
<h3 id="解决问题">解决问题</h3>
<p>好吧，调整下settings。这里最重要的就是这句&quot;index.mapping.total_fields.limit&quot;: 2000(默认值是1000)。当然这里需要考虑下对于开卡这种日志，其实有必要让他自己单独享用一个index，来减少这种问题。</p>
<pre><code>{
    &quot;index.mapping.total_fields.limit&quot;: 2000,
}
</code></pre>
<p>由于一般我们会使用动态模板，这样我们每次的设置仅对当前的索引产生效果。如果第二天来的日志，还会使用默认的模板来进行索引。最好的方案是使用自定义模板:</p>
<pre><code class="language-json">put {{domain}}/_template/kakiraid-logs

{
  &quot;template&quot;: &quot;kakiraid-*&quot;, 
  &quot;order&quot;:    1, 
  &quot;settings&quot;: {
    &quot;index.indexing.slowlog.threshold.index.debug&quot; : &quot;2s&quot;,
    &quot;index.indexing.slowlog.threshold.index.info&quot; : &quot;5s&quot;,
    &quot;index.indexing.slowlog.threshold.index.trace&quot; : &quot;500ms&quot;,
    &quot;index.indexing.slowlog.threshold.index.warn&quot; : &quot;10s&quot;,
    &quot;index.mapping.total_fields.limit&quot;: 3000,
    &quot;index.merge.policy.max_merged_segment&quot; : &quot;5gb&quot;,
    &quot;index.merge.policy.segments_per_tier&quot; : &quot;24&quot;,
    &quot;index.merge.scheduler.max_merge_count&quot; : 10,
    &quot;index.merge.scheduler.max_thread_count&quot; : 1,
    &quot;index.number_of_shards&quot;: &quot;5&quot;,
    &quot;index.number_of_replicas&quot; : &quot;1&quot;,
    &quot;index.optimize_auto_generated_id&quot; : &quot;true&quot;,
    &quot;index.refresh_interval&quot; : &quot;60s&quot;,
    &quot;index.routing.allocation.total_shards_per_node&quot; : &quot;-1&quot;,
    &quot;index.search.slowlog.threshold.fetch.debug&quot; : &quot;500ms&quot;,
    &quot;index.search.slowlog.threshold.fetch.info&quot; : &quot;800ms&quot;,
    &quot;index.search.slowlog.threshold.fetch.trace&quot; : &quot;200ms&quot;,
    &quot;index.search.slowlog.threshold.fetch.warn&quot; : &quot;1s&quot;,
    &quot;index.search.slowlog.threshold.query.debug&quot; : &quot;2s&quot;,
    &quot;index.search.slowlog.threshold.query.info&quot; : &quot;5s&quot;,
    &quot;index.search.slowlog.threshold.query.trace&quot; : &quot;500ms&quot;,
    &quot;index.search.slowlog.threshold.query.warn&quot; : &quot;10s&quot;,
    &quot;index.translog.durability&quot; : &quot;async&quot;,
    &quot;index.translog.flush_threshold_size&quot; : &quot;5000mb&quot;,
    &quot;index.translog.sync_interval&quot;: &quot;600s&quot;,
	&quot;index.unassigned.node_left.delayed_timeout&quot; : &quot;7200m&quot;
  },
  &quot;mappings&quot;: {
    &quot;_default_&quot;: { 
	  &quot;dynamic&quot;: true,
      &quot;_all&quot;: {
        &quot;enabled&quot;: false
      },
      &quot;dynamic_templates&quot;: [
        {
          &quot;string_template&quot;: {
            &quot;match_mapping_type&quot;: &quot;string&quot;,  
            &quot;match&quot;: &quot;*&quot;,                    
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;text&quot;,
              &quot;fields&quot;: {
                    &quot;keyword&quot;: {
                        &quot;type&quot;: &quot;keyword&quot;,
                        &quot;ignore_above&quot;: 256
                    }
                }
            }
          }
        },
        {
          &quot;ip&quot;: {
            &quot;match_mapping_type&quot;: &quot;*&quot;,
            &quot;match&quot;: &quot;*_ip&quot;,
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;ip&quot;
            }
          }
        },
        {
          &quot;date&quot;: {
            &quot;match&quot;: &quot;*_date&quot;,            
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;date&quot;,            
              &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot;  
            }
          }
        },
        {
          &quot;keyword&quot;: {
            &quot;match_mapping_type&quot;: &quot;string&quot;,
            &quot;match&quot;: &quot;*_k&quot;,                 
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;keyword&quot;,
              &quot;ignore_above&quot;: 256,
              &quot;null_value&quot;: &quot;null&quot;
            }
          }
        },
        {
          &quot;long&quot;: {
            &quot;match_mapping_type&quot;: &quot;*&quot;,
            &quot;match&quot;: &quot;*_l&quot;,              
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;integer&quot;          
            }
          }
        },
        {
          &quot;double&quot;: {
            &quot;match_mapping_type&quot;: &quot;*&quot;,
            &quot;match&quot;: &quot;*_d&quot;,              
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;double&quot;           
            }
          }
        },
        {
          &quot;boolean&quot;: {
            &quot;match_mapping_type&quot;: &quot;boolean&quot;,
            &quot;match&quot;: &quot;*_b&quot;,                
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;boolean&quot;
            }
          }
        },
        {
          &quot;analyzer&quot;: {
            &quot;match&quot;: &quot;*_a&quot;,                
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;text&quot;,                
              &quot;index&quot;: true,
              &quot;analyzer&quot;: &quot;english&quot;,        
              &quot;null_value&quot;: &quot;null&quot;
            }
          }
        }
      ]
    }
  }
}
</code></pre>
<p>其他的是我为了加快索引速度给的一些“调优”。比如refresh_interval这个默认值是1s，虽然看上去日志是所见即所得，但有的时候并不需要这么快。可以适当的延长这个值，让日志在file_cache中多呆一会儿再被可见。其实对于elk来说，文档可见其实只要缓存到了file_cache中即可，并不需要写入到硬盘。</p>
<p>当然通过flush也是可以保证这点的，但是它的代价就是磁盘io。所以我这里将translog设定成了异步并且有大小阈值的。因为这些日志文件并不是那么的重要。</p>
<p>同时我还设定了dynamic_templates来保证未来的扩展性，只要符合命名规则就可以自动解析成想要的类型了。这里要注意得是模板其实也有顺序的，个人越是通用的应该要放在上面比较好。然后对于string类型来说，建议还是使用text而不是keyword，这样会使得日常查看日志方便许多。</p>
<h3 id="如何重新装载日志呢">如何重新装载日志呢？</h3>
<p>为了保证日志的完整性，肯定是需要重新index受到影响的日志。这个结合filebeat的inode特性，我们首先需要隔离开受到“错误”影响的日志，比如某个时间段之后的日志的索引都有问题。按照我的经验是截至到当天的前一天为好。最重要的原因是当天的index还没有完成，处于正在执行状态，是不会接受已经被索引过的日志的。要么就是停止当天的索引，要么就是隔天操作。</p>
<p>隔离开的日志只要更换下inode，然后就可以做到不停机的情况下重新index了。比较稳妥的方案是可以删除掉已经存在Index, 然后找一个压力比较小的时候做以上的操作:)</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[logstash 如何处理复杂日志]]></title>
        <id>https://xiwan.github.io/post/6SadtR1zT</id>
        <link href="https://xiwan.github.io/post/6SadtR1zT">
        </link>
        <updated>2019-12-26T10:30:44.000Z</updated>
        <summary type="html"><![CDATA[<p>最近压测发现日志在抽奖接口会放大百倍，直接把10万长度的日志队列挤爆！</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近压测发现日志在抽奖接口会放大百倍，直接把10万长度的日志队列挤爆！</p>
<!-- more -->
<p>深入调查后发现，抽奖接口的逻辑是得到一个物品就会对应一条日志。然后经常用户会大量开卡包的情况下，会单次获得许多物品，这样照成了日志的放大。压测时候每次请求会产生261条日志！！！</p>
<p>第一个想到的解决方案是日志合并，在抽奖的最后再写一条日志。不过这带来新的挑战，日志的维度会增加。导致现阶段的分析方式都无法使用了。</p>
<p>举例来说，对于这样一条日志，它是以分隔符竖线来区别每个字段意思的：</p>
<pre><code>2019-12-24 23:59:59|9|116504|76561198271728609@STEAM|4|86|9|127|32|-1|4||10011|1462231
</code></pre>
<p>这样我们在logstash里面处理时候简单的split结果，然后按照位置即可获得字段的意思。一次抽卡有可能产生多条这样的日志，所以上面的处理方案就会合并成</p>
<pre><code>2019-12-26 17:11:29|9|10001889532|bot1234567890|0|16|1617,87|1504^1,1551^1,1536^1,1539^1,1511^1,1503^1,1533^1,1552^1,1535^1,1504^1,1505^1,1528^1,1542^1,1519^1,1533^1,1525^1,1502^1,1503^1,1503^1,1502^1,1534^1,1545^1,1502^1,1502^1,1502^1,1505^1,1526^1,1510^1,1503^1,1549^1,1531^1,1509^1,1501^1,1526^1,1551^1,1501^1,1530^1,1504^1,1551^1,1548^1,1513^1,1521^1,1554^1,1511^1,1526^1,1551^1,1508^1,1529^1,1514^1,1516^1,1536^1,1526^1,1528^1,1554^1,1526^1,1551^1,1524^1,1541^1,1536^1,1502^1,1539^1,1549^1,1505^1,1511^1,1504^1,1539^1,1501^1,1501^1,1529^1,1529^1,1503^1,1551^1,1509^1,1519^1,1541^1,1528^1,1514^1,1507^1,1548^1,1512^1,1534^1,1523^1,1501^1,1501^1,1503^1,1534^1,1538^1,1529^1,1505^1,1517^1,1531^1,1536^1,1505^1,1539^1,1547^1,1504^1,1540^1,1504^1,1551^1,1535^1,1505^1,1519^1,1518^1,1504^1,1526^1,1525^1,1504^1,1501^1,1550^1,1542^1,1529^1,1554^1,1509^1,1552^1,1554^1,1512^1,1501^1,1521^1,1515^1,1525^1,1535^1,1552^1,1510^1,1527^1,1507^1,1523^1,1516^1,1513^1,1538^1,1551^1,1502^1,1536^1,1501^1,1506^1,1533^1,1543^1,1527^1,1539^1,1521^1,1513^1,1540^1,1504^1,1503^1,1530^1,1549^1,1552^1,1545^1,1553^1,1501^1,1536^1,1533^1,1551^1,1511^1,1502^1,1502^1,1529^1,1536^1,1549^1,1508^1,1552^1,1503^1,1533^1,1540^1,1512^1,1524^1,1501^1,1541^1,1536^1,1505^1,1503^1,1528^1,1523^1,1551^1,1513^1,1504^1,1527^1,1539^1,1512^1,1551^1,1550^1,1552^1,1507^1,1528^1,1536^1,1552^1,1505^1,1552^1,1548^1,1530^1,1536^1,1523^1,1509^1,1541^1,1503^1,1503^1,1533^1,1505^1,1505^1,1508^1,1501^1,1535^1,1543^1,1508^1,1523^1,1536^1,1514^1,1542^1,1547^1,1505^1,1504^1,1539^1,1503^1,1503^1,1502^1,1551^1,1525^1,1545^1,1516^1,1522^1,1543^1,1552^1,1501^1,1503^1,1504^1,1532^1,1531^1,1505^1,1518^1,1536^1,1552^1,1501^1,1551^1,1509^1,1535^1,1520^1,1511^1,1539^1,1523^1,1514^1,1504^1,1546^1,1515^1,1504^1,1527^1,1529^1,1524^1,1547^1,1534^1,1548^1,1525^1,1501^1,1552^1,1547^1,1548^1,1527^1,1501^1,1551^1,1546^1,1504^1,1552^1,1547^1||10021|199
</code></pre>
<p>对的，你没看错也没有搞错！一下子日志维度就上去了。</p>
<h3 id="改造logstash">改造logstash</h3>
<p>由于单纯的写logstash的split似乎无法完成上面的任务了（主要问题是不定长array处理起来十分麻烦），所以我对准了它的ruby插件:</p>
<pre><code>ruby {
        code =&gt; '
          item_array=event.get(&quot;[log_json][t9_item_id]&quot;).split(&quot;,&quot;)
          event.set(&quot;[log_json][t9_item_length]&quot;, item_array.length)
          for item in item_array
            value=event.get(&quot;[log_json][t9_item_#{item.split(&quot;^&quot;)[0]}]&quot;)
            if value.nil?
              event.set(&quot;[log_json][t9_item_#{item.split(&quot;^&quot;)[0]}]&quot;, item.split(&quot;^&quot;)[1].to_i)
            elsif
              event.set(&quot;[log_json][t9_item_#{item.split(&quot;^&quot;)[0]}]&quot;, item.split(&quot;^&quot;)[1].to_i+value.to_i)
            end
          end
        '
      }
</code></pre>
<p>可以看到在ruby内部, 每一条日志其实是一个event. 通过内置的event.set 和 get方法就可以很简单的将日志继续分割下去。</p>
<p>最后得到的结果为：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1577357078972.png" alt=""></figure>
<p>当然我们也关系加入新的ruby插件后的效率问题。找了台4G，2核的普通机器在没有调优参数的情况下，pv 压测后的结果为:</p>
<p>4核4GB的机器结果** 978KiB 0:05:43 [2.85KiB/s]**差不多100万条复杂数据，每秒2.85k个event的速度</p>
<p>这里有个比较有意思的收获，原来获取array的尾部在conf里面可以写-1...</p>
<h3 id="elk查询">elk查询</h3>
<p>上面基本解决了logstash在多维度结构时候的解析问题，对于elk中一个按照物品id的聚合查询该如何做呢？</p>
<p>我想的思路是： 传入item_id, 匹配日志name, 由于value是记录了数目，那么query里面的条件则是Numeric Range Query Usage来进行比较（大于0即可）。在aggregator里面的则是用总和sum操作即可。</p>
]]></content>
    </entry>
</feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://xiwan.github.io</id>
    <title>Keep Thinking</title>
    <updated>2020-05-27T10:21:18.246Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://xiwan.github.io"/>
    <link rel="self" href="https://xiwan.github.io/atom.xml"/>
    <subtitle>有美人兮心不怿</subtitle>
    <logo>https://xiwan.github.io/images/avatar.png</logo>
    <icon>https://xiwan.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Keep Thinking</rights>
    <entry>
        <title type="html"><![CDATA[制作带有dotnet的jenkins(dotnet3.1)]]></title>
        <id>https://xiwan.github.io/post/4bhipKYFu</id>
        <link href="https://xiwan.github.io/post/4bhipKYFu">
        </link>
        <updated>2020-05-27T10:16:40.000Z</updated>
        <content type="html"><![CDATA[<h3 id="dotnet安装脚本-dotnet-install-31sh">dotnet安装脚本 dotnet-install-3.1.sh</h3>
<p>可以根据需要换成其他版本</p>
<pre><code># Based on instructiions at https://www.microsoft.com/net/download/linux-package-manager/debian9/sdk-current
# Install dependency for .NET Core 2
apt-get update
apt-get install -y curl libunwind8 gettext apt-transport-https

# Based on instructions at https://www.microsoft.com/net/download/linux-package-manager/debian9/sdk-current
# Install microsoft.qpg &amp; Install the .NET Core framework 

wget -O- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.asc.gpg
mv microsoft.asc.gpg /etc/apt/trusted.gpg.d/
wget https://packages.microsoft.com/config/debian/9/prod.list
mv prod.list /etc/apt/sources.list.d/microsoft-prod.list
chown root:root /etc/apt/trusted.gpg.d/microsoft.asc.gpg
chown root:root /etc/apt/sources.list.d/microsoft-prod.list

apt-get update
apt-get install -y dotnet-sdk-3.1
apt-get install -y zip
apt-get clean
</code></pre>
<h3 id="dockerfile部分">Dockerfile部分</h3>
<pre><code>FROM jenkins/jenkins:lts
# Switch to root to install .NET Core SDK
USER root
# Show distro information!
RUN uname -a &amp;&amp; cat /etc/*release
RUN apt-get update &amp;&amp; apt-get install -y libltdl7

ARG dockerGid=999
RUN echo &quot;docker:x:${dockerGid}:jenkins&quot; &gt;&gt; /etc/group

COPY dotnet-install-3.1.sh install.sh
RUN bash install.sh

# Switch back to the jenkins user.
USER jenkins
</code></pre>
<h3 id="终端输入">终端输入</h3>
<pre><code class="language-shell">sudo chown -R 1000:1000 /var/docker_data/jenkins/jenkins_home

# 制作镜像
docker build . -t jenkins-dotnet

# 启动镜像
# 注意端口映射规则 其中18080为jenkins访问端口 50000为底层tcp端口

docker run --name kaki-jenkins-dotnet -p 18080:8080 -p 50000:50000 -v /var/docker_data/jenkins/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v /volume1/docker/jenkins/backups/jenkins-153:/var/backups -v /volume1/Outofbag/server:/var/server -v $(which docker):/usr/bin/docker -e JAVA_OPTS=-Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Shanghai -d jenkins-dotnet

</code></pre>
<h4 id="docker-composeyml">docker-compose.yml</h4>
<p>对应docker-compose文件</p>
<pre><code class="language-yaml">version: '2'
services:
  jenkins-core:
    build: ./
    environment:
      - JAVA_OPTS=-Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Shanghai 
    ports:
      - 18080:8080
      - 50000:50000
    # uncomment for docker in docker
    privileged: true
    restart: always
    volumes:
        # enable persistent volume (warning: make sure that the local jenkins_home folder is created)
        - /var/docker_data/jenkins/jenkins_home:/var/jenkins_home
        # mount docker sock and binary for docker in docker (only works on linux)
        - /var/run/docker.sock:/var/run/docker.sock
		- /volume1/docker/jenkins/backups/jenkins-153:/var/backups
        - /usr/local/bin/docker:/usr/bin/docker
		- /volume1/Outofbag/server:/var/server
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[支付相关的]]></title>
        <id>https://xiwan.github.io/post/m58UGVTjO</id>
        <link href="https://xiwan.github.io/post/m58UGVTjO">
        </link>
        <updated>2020-05-22T08:10:20.000Z</updated>
        <summary type="html"><![CDATA[<p>最近接了不少支付相关的sdk，自己看下来发现这个业务其实蛮固定的套路。这里做下总结</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近接了不少支付相关的sdk，自己看下来发现这个业务其实蛮固定的套路。这里做下总结</p>
<!-- more -->
<h2 id="介绍">介绍</h2>
<p>其实市面上是有各种不同的支付平台的，像国内有aliy, wechat, 国外是paypal。 这还是比较主流的平台，如果考虑到其他的渠道，毛估可能会有上百个。</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1590135265757.png" alt=""></figure>
<p>上面这个截图是从payssion平台截图下来的。对于开发者来说要全部接入一遍明显不现实的，并且也受限于法律法规有些平台不是你想接就能接入的。</p>
<h2 id="流程">流程</h2>
<p>其实如果自己大概能接2-3个以后就会发现其实这些支付业务是差不多的“套路”的，或者也叫做流程。</p>
<pre><code>1. 去对应平台生成应用，最主要的是拿到appId, appKey, appSecret这三样东西。
    1. appId一般是是用来识别你应用的（非必须）
    2. appKey一般是用于签名（可以认为是公钥），可暴露
    3. appSecret也是用于签名（可以认为是私钥），不可暴露
2. 客户端像业务服务器拉起订单请求，主要是获取订单号orderId和金额amount
3. 客户端带着前面的获取的信息，继续请求业务服务器拉起支付请求。
4. 支付请求需要进心签名，各个平台签名方案不一样。但基本上是将一些必须参数同appKey和appSecret混合起来进行签名，获得一个sig。
    1. 常见的一种签名方式（服务器端）：将业务参数按照字母顺序排列（除掉appKey），然后用appkey进心md5之类签名获得sig。当然这中间会涉及一些特殊字符和大小写处理。
    2. 渠道一般会用私钥去解，看能不能获得相同的sig来判断签名是否正确。
5. 支付请求还需要带着两个额外的参数： returnUrl和notifyUrl。 这两个参数也可以通过渠道网站后台设置。
    1. returnUrl可以认为是支付完成后跳转位置
    2. notifyUrl实际上是一个异步状态同步通知位置
6. 一般来说，通过sdk完成步骤5后，返回结果会有一个和orderId匹配的transactionId。这个东西就是平台的流水号，而orderId是业务的流水号。两者需要建立一个映射关系，比如能够互相查找之类。像国内的基本会返回一个二维码支持移动支付。
7. 基本用户完成支付后，业务网站就是等待渠道调用notifyUrl就好了。为了避免丢失请求或者网络问题，渠道会要求notifyUrl返回特定字符，比如'success'。如果没有收到，它会通过轮询的方式去继续请求这个notifyUrl。最后的结果是要么获取到了特定字符，要么尝试上限达到而失败。
8. 收到NotifyUrl请求，首先也要验证平台过来sig的有效性，然后业务逻辑需要进行一个去重或者“等幂”操作。主要是更新订单状态state和进行商品发放的逻辑。
9. 在notifyUrl也有可能失败的情况下，不少渠道是提供query的功能的。这样业务可以通过主动查找订单的状态来继续自己的业务逻辑。
</code></pre>
<p>其实可以看到整个流程还是比较清晰的，就算有平台的差异性，但这个“套路”是基本固定的。有了这个固定的“套路”，其实诞生了不少的“中间商”： 它们做的事情无非是帮你统一了各个平台的差异化：</p>
<pre><code>1. 统一了参数名字
2. 同意了接口路由
3. 统一了签名方法
4. 统一了汇率之类其他东西
</code></pre>
<p>然后收取了一些“服务费”。感觉这些公司就是躺着赚钱啊，真是容易。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[族群架构进化论]]></title>
        <id>https://xiwan.github.io/post/fu_2s9jJ4</id>
        <link href="https://xiwan.github.io/post/fu_2s9jJ4">
        </link>
        <updated>2020-04-24T08:09:34.000Z</updated>
        <summary type="html"><![CDATA[<p>最近再调整游戏服务器的架构，有了些感悟，趁着热乎先记录下来。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近再调整游戏服务器的架构，有了些感悟，趁着热乎先记录下来。</p>
<!-- more -->
<h2 id="目前架构">目前架构</h2>
<p>首先祭出一张目前的简单架构图：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1587718706319.png" alt=""></figure>
<p>基本流程是：<br>
1. 用户先连接CDN，获取可用服务器列表<br>
2. 基于算法随机到其中一台游戏服务器（长连接）<br>
3. 游戏服务器内部交互都通过Redis的订阅模式完成。<br>
4. 保留一个Hub服务器，处理全局任务（排行榜之类）</p>
<p>分析下这个结构的好处：<br>
1. 扩展简单，对于游戏压力来说，只要CDN不跨掉就可以了。<br>
2. 跨服经过redis，这个东西比较成熟，使用起来也很简单<br>
3. 业务划分简单，玩家部分走GS, 系统部分走Hub.</p>
<p>当然也有一定的问题：<br>
1. Hub有可能提前成为瓶颈<br>
2. Redis本身无法保存消息</p>
<h2 id="优化版本">优化版本</h2>
<p>这里我准备用一个专门的socket服务器来替代redis，不关注任何业务，只是做简单的转发。这就是前面提到的调节服务器(Intermediary)。它会支持两种转发策略：<br>
1. 点到点：业务对状态敏感，直接转发。<br>
2. 随机点：业务对状态并不敏感，随机转发。</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1587720969390.png" alt=""></figure>
<p>这里虚线框中的服务器可以按照业务组成一个<strong>族群</strong>，那么这个<strong>族群的首领</strong>就是调节服务器。它扮演的角色就是：<br>
1. 统一收发对外消息<br>
2. 内部协调消息扭转<br>
3. 消息状态的维护</p>
<p>由于Hub变成了无状态的，所以它同样和GS一样十分方便的可以扩展。</p>
<h2 id="扩展版本">扩展版本</h2>
<p>其实我们可以把整个服务器架构都按照这种<strong>族群</strong>的思路来设计。其中调节服务器就类似于“网关”。他们是进出整个<strong>族群</strong>的出入口。那么我们可以看到一个最简单的服务器就是一个<strong>族群</strong>。当我们需要它按照业务无限扩容时候，就变成如下的一个拓扑结构：</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1587721806494.png" alt=""></figure>
<p>这里的业务也是多种定义：可以是普通的<strong>游戏逻辑族群</strong>，也可以是专门的<strong>战斗族群</strong>，或者是<strong>任务族群</strong>，甚至是<strong>存储族群</strong>都可以的。</p>
<p>可以看到这套架构是比较松耦合的，大家都是靠通信协议在<strong>协调服务器</strong>通信。新的业务加入和去除也比较方便。</p>
<p>可能这里对于<strong>协调服务器</strong>的单点情况是有点担心的。我这里考虑到的做法：<br>
1. 利用虚地址或者一致性hash之类保证它的高可用（横向）<br>
2. 提升本身机器的性能（纵向）<br>
3. 根据业务动态的裂变或者合并这个节点</p>
<p>前面两点比较容易理解。第三点的意思是，比如我很多业务都走Hub服务器族群，让它的协调服务器变得压力很大，这时我们可以考虑拆分这些业务：比如把排行榜和定时任务拆开。这样就变成了两个族群。同理，反向操作也是可以的。</p>
<p>关系图如下：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1587722838681.png" alt=""></figure>
<h2 id="总结">总结</h2>
<p>可以看到整个架构的一个进化过程其实是很自然也很动态的。但需要配合很多CI/CD的工具来达到这个目的。不过后面我会先用netmq来实现这个框架。</p>
<h3 id="参考">参考</h3>
<p>https://netmq.readthedocs.io/en/latest/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[用一首歌的时间落地10万个用户数据]]></title>
        <id>https://xiwan.github.io/post/21M9ANIq1</id>
        <link href="https://xiwan.github.io/post/21M9ANIq1">
        </link>
        <updated>2020-03-25T07:47:42.000Z</updated>
        <summary type="html"><![CDATA[<p>为啥是一首歌，因为新的需求是极端情况可以忍受玩家回档5分钟。哪些极端情况呢？redis完全不能用了，只能走mysql来落地。来保护玩家数据。</p>
<p>不过有个明显的问题是：mysql到底能不能抗此大责任？</p>
]]></summary>
        <content type="html"><![CDATA[<p>为啥是一首歌，因为新的需求是极端情况可以忍受玩家回档5分钟。哪些极端情况呢？redis完全不能用了，只能走mysql来落地。来保护玩家数据。</p>
<p>不过有个明显的问题是：mysql到底能不能抗此大责任？</p>
<!-- more -->
<h3 id="介绍">介绍</h3>
<p>首先我们得知道每个用户大概占多少内存。根据线上用户的统计来看，差不多可以认为上限为200kb</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1585122972394.png" alt=""></figure>
<p>如果我们要保存10万个用户，差不多就是100,000x200Kb = 20G。这个对redis来说已经算比较大的压力了。所以自然考虑到了mysql。</p>
<p>mysql在这里主要扮演的是备份和还原的角色</p>
<ul>
<li>备份：定时把内存中的用户信息写入mysql，对时间比较敏感，同时需要保证数据的完整性。</li>
<li>还原：回滚用户数据时候，停服将mysql里面数据加载到redis，对时间不是特别敏感。</li>
</ul>
<h3 id="做法">做法</h3>
<p>比较常规的做法是用一个循环套住insert或者update语句，如果10万个用户差不多需要跑10次。比较直接，但反复操作数据库连接太消耗，故没有考虑。</p>
<p>查询资料后，发现还有四种技巧：</p>
<ol>
<li>insert into ...on duplicate key update批量更新</li>
<li>replace into 批量更新</li>
<li>创建临时表，先更新临时表，然后从临时表中update</li>
<li>使用mysql 自带的语句构建批量更新</li>
</ol>
<p>进过研究，比较适合我这个项目的是方法2. 1和4主要是构建的sql语法比较麻烦，并且将业务逻辑下沉到数据库本身不是一种提倡的做法。3的开销主要是建立表，如果中间出现数据库问题，也是代价比较大。</p>
<h3 id="迭代">迭代</h3>
<p>由于内存限制，不大可能把10万个用户都加载到内存。所以准备加载1000个用户，然后循环100次。</p>
<p>第一个版本比较直接，就是将内存中所有用户全部构建成sql语句来执行。测试速度如下：</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1585124385650.png" alt=""></figure>
<p>可以看到似乎不错，差不多10分钟内可以完成整个过程。但是离我们的目标5分钟还是有距离。</p>
<p>第二个版本：我固定了每次执行的用户数目，比如500个。这样改造后的速度：</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1585124674444.png" alt=""></figure>
<p>已下子到了6分钟。看来批量定长是一个不错的方向。</p>
<p>第三个版本：由于数据本身200kb一个有压缩空间，所以我把每个用户数据进行了gzip压缩再存储。这次到达速度为：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1585124866510.png" alt=""></figure>
<p>结果不错，已经到达5分钟级别了。那么还能不能更快呢？我做了第四个版本是多线程的，但结果不是特别好：</p>
<figure data-type="image" tabindex="5"><img src="https://xiwan.github.io/post-images/1585125335885.png" alt=""></figure>
<p>这个版本基本和第三版本持平，唯一的好处做成一个异步的过程。当然也有可能是我实现得不对:)</p>
<h3 id="如何进一步挖掘潜力">如何进一步挖掘潜力</h3>
<p>似乎单机10万写入速度已经到了极限，提升不大了。这个时候比较好的思路是拆表了，加入我把10万个用户拆成5个表，每个表只负责2万人。相信整体速度会有个更大的提升。</p>
<p>由于游戏的规模可能就是10万人同时在线，所以每台服务器内存很可能就是只负责差不多这10万人的一部分。那么为什么不直接用一张表来放呢？</p>
<p>其实这里基于两点考虑：<br>
1.多个线程写一张表可能会出现竞争甚至死锁的问题。<br>
2.然后玩家可能会在多个服务器都有记录，这个时候由于同步机制的问题，很可能老的数据反而覆盖了新的数据问题。</p>
<p>当然分成多个表带来的问题是如何识别重复数据哪份更新。最简单的做法是使用时间戳， 不过考虑到机器的时间差，其实用时间戳不是一个特别好的方案。还是需要使用redis或者zookeeper加锁机制来获取一个递增的版本号来管理。当发现两份数据的key值相同时候，通过对比版本号大小即可。</p>
<h3 id="结语">结语</h3>
<p>这次是基于C#的Dapper库做的实验，其他的链接库应该差不多。主要是这一通操作的思想：尽量把相同的数据批处理，然后是定长，再配合压缩和异步的技术，那么这么大的数据也是可以写入mysql的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一种防止客户端作弊的方案]]></title>
        <id>https://xiwan.github.io/post/l4Y3atzIB</id>
        <link href="https://xiwan.github.io/post/l4Y3atzIB">
        </link>
        <updated>2020-03-02T09:57:39.000Z</updated>
        <summary type="html"><![CDATA[<p>个人觉得要防住外挂，最好的方案就是一切走服务器来验证。尤其是一些关键性的数据，比如血条，装备和位置。</p>
]]></summary>
        <content type="html"><![CDATA[<p>个人觉得要防住外挂，最好的方案就是一切走服务器来验证。尤其是一些关键性的数据，比如血条，装备和位置。</p>
<!-- more -->
<p>这里我以位置来举例，如果什么时候都要把数据同步给服务器来验证后再移动，除了可能的丢包之外，来回的开销也会让客户端玩起来像PPT一样。当然回合制的游戏，比如卡牌之类可以考虑这种方案。如果是对实时性要求比较高的FPS游戏，肯能就无法忍受这种方案了。</p>
<h3 id="模型思路">模型思路</h3>
<p>有一种比较实用的方案是：由于“位置”这个信息比较敏感，客户端会不停地反复更新它。肯定是需要服务器校验的。所以可以去掉客户端的直接写它的权利，转而做一个“位置副本”。客户端对于位置的更新都给予这个“位置副本”，包括读和写。这样由于操作都在客户端所以游戏的流畅感会好很多。</p>
<p>对于原来的“位置”数据，客户端只保留了读的权利。相反，服务器将拥有“位置”数据的读和写权利，但对于“位置副本”却只有读的权利。</p>
<p>那么当客户端不断的操作“位置副本”，并且不停地将信息同步给服务器的时候。服务器可以拿“位置”数据和它进行交叉对比。如果发现符合<strong>算法条件</strong>的下移动，则可以拿“位置副本”更新“位置”数据。相反，如果发现异常：比如一下子穿墙之类，服务器就不会更新“位置”数据。并且异步告知客户端需要修正它的“位置副本”。</p>
<p>通常客户端是只要关注“位置副本”数据的，一旦收到服务器的“位置”异常信息。则需要立马修正自己的“位置副本”到上一个正确的“位置”数据。这样就达到了一个既能保证流畅性又能完成校验的目的了。</p>
<h3 id="服务器预估">服务器预估</h3>
<p>那么服务器如何判定客户端的“位置副本”正确性呢？</p>
<p>首先， 服务器需要一份地图数据（这份地图数据有可能也需要实时更新，比如在一些可以破坏地形的场景中）。同时他需要保存一份所有玩家的“位置”信息。这样服务器就知道玩家某个时刻在地图的哪个位置了。当它收到客户端的“位置副本”数据更新信息后，除了做一些常规的校验外。最终要的是去匹配当前值是否和“预估位置”值。如果符合，则可以更新。</p>
<p>那么这个“预估位置”如何得到呢？这里会涉及比较多的因素： 比如玩家的三维值，环境值之类。基本上的思路是基于上一个“位置”数据，然后穷尽游戏内所有可以对移动产生的作用的因素，并把它转化成许多个对应的“向量”，然后进行相加就可以获得“预估位置”了。</p>
<p>当然基于现实情况的复杂性，要求“位置副本”和“预估位置”严格相等是显然不现实的。所以还要容忍一定的“合理误差”。</p>
<h3 id="客户端呢">客户端呢？</h3>
<p>由于我不是客户端出生，所以思路有可能比较局限。现在想到的方案有，进行包混淆可以一定层度的减少被破解。然后一些敏感的“位置”信息相关的逻辑可以欺骗性的用字符串而不是常规的数字，这样就可以避免被内存定位到之类。当然肯定不止这些方案。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[调查redis的protocal error]]></title>
        <id>https://xiwan.github.io/post/O3_DlNeqo</id>
        <link href="https://xiwan.github.io/post/O3_DlNeqo">
        </link>
        <updated>2020-02-18T10:43:42.000Z</updated>
        <summary type="html"><![CDATA[<p>去年年底封测的第一个晚上，晚高峰时候，本来一切平和。但是在毫无征兆的情况下，突然线上redis断了。检查了机房连接没有问题，并且线上10多个实例，只有一台机器的redis断掉了。十分诡异！排查了一个小时没有任何结果，此次问题造成了大概800人回档。对于当时近万人的同时在线，还算损失比较小。</p>
]]></summary>
        <content type="html"><![CDATA[<p>去年年底封测的第一个晚上，晚高峰时候，本来一切平和。但是在毫无征兆的情况下，突然线上redis断了。检查了机房连接没有问题，并且线上10多个实例，只有一台机器的redis断掉了。十分诡异！排查了一个小时没有任何结果，此次问题造成了大概800人回档。对于当时近万人的同时在线，还算损失比较小。</p>
<!-- more -->
<p>这个错误在以前的测试，包括压测过程中都没有出现过。并且steam服务器跑了也将近半年没有问题。但当时简单修复相关业务逻辑后，这个错误在封测期间再也没有问题了。所以又搁置了起来。</p>
<p>不过封测停止后，这个问题突然在steam服务器也多了起来。辛亏的是当时做了一个保护措施（一旦发现redis连接断掉就立马重连）。所以还算安稳度过到现在。</p>
<h3 id="如何稳定复现这个问题">如何稳定复现这个问题？</h3>
<p>现在摆在手上的日志只有failed to write时候一定会断线，于是找到我们使用的这个第三方客户端源码研读起来：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1582108891413.png" alt=""></figure>
<p>这就是抛错的地方，的确是连接问题。但是什么条件会抛呢？由于源码实在复杂，所以这部分并没有继续下去。</p>
<h3 id="继续深挖">继续深挖</h3>
<p>由于最近几乎每天redis都会自动断掉然后重连，于是决定直接monitor现场。这个命令的作用就是看redis当时在处理什么命令。具体可以参考官方文档。结果发现监控不多久就出现了如下错误：</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1582109109573.png" alt=""></figure>
<p>然后同样也是连接断掉了。当然redis自己还是稳定运行的。在网上查了下相关问题，很多人给的答案有点让人捉摸不透：</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1582109233761.png" alt=""></figure>
<p>因为问题是出在客户端这边，为什么要修改redis服务器的一些配置甚至去升级呢？就感觉这个做法有点像<strong>头痛医脚</strong>。</p>
<h3 id="柳暗花明">柳暗花明</h3>
<p>就在一筹莫展时候，我阅读了官网的<a href="https://xiwan.github.io/post/FwWrD9HpO/">协议文章</a></p>
<p>于是有了个推论会不会是底层的byte乱了。比如说我多了个CRLF或者少了个CRLF之类。于是做了些测试，的确验证了我的想法：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1582112369686.png" alt=""></figure>
<p>当出现<strong>protocol error</strong>后我自己写的客户端也被断掉了。虽然这个错误和monitor看到的错误并不一样，但看错误日志可以得出他们都属于<strong>protocol error</strong>这个大类型。</p>
<pre><code>当redis抛出protocol error后会立刻关闭客户端的socket
</code></pre>
<p>上面的是我做出的一个结论。</p>
<h3 id="继续测试">继续测试</h3>
<p>有了上面那个结论，我怎么去做测试数据来验证这点呢？因为协议的部分是底层实现不大可能多一个或者少一个<strong>关键字符</strong>。于是我想着自己做一些数据来验证这点。</p>
<figure data-type="image" tabindex="5"><img src="https://xiwan.github.io/post-images/1582109822145.png" alt=""></figure>
<p>程序逻辑大概是随机组合这些保留字符窜丢给redis，不过截至目前为止，我也没有试验出来断线。看样子就协议本身来说还是比较稳定的，只要底层协议按照官方的来，哪怕你输入了一些特殊字符， redis本身还是能顺利处理的。</p>
<p>到了这里，似乎又陷入了死胡同。。。</p>
<h3 id="回到原点">回到原点</h3>
<p>既然错误是redis抛出来的，于是我回到了它的源码上面。很快就找到了一些端倪了：</p>
<figure data-type="image" tabindex="6"><img src="https://xiwan.github.io/post-images/1582110076823.png" alt=""></figure>
<p>这段代码很明显是处理首字符的时候发现了未定义字符而抛错，进而redis认为连接的字符顺序错误了，从而导致了断线。并且有意思的是这段错误是来自hiredis而非redis服务器本身，另一个有意思的是老版本显然支持的协议字符要比上面的更少。</p>
<p>hiredis是redis作者写的一个客户端，可以推测使用的redis-cli命令应该是基于这个开发的，所以用monitor命令时候才会发现这个错误。看样子是回包时候，发现了数据错误。那么这个错误似乎和我们线上问题无关了。因为Failed to write明显是写的时候错误了。</p>
<h3 id="暂时总结">暂时总结</h3>
<ol>
<li>redis命令由于定义了一套自己的协议，第三方协议需要按照它来和redis交互。</li>
<li>出现protocol error必然会断线，所以一定要做好redis连接的保护。</li>
<li>protocol error错误出现后，要关注后面的内容。有些事hiredis发出来的，有些是redis发出来的。</li>
<li>网上说得更改redis设置可以解决protocal错误明显是误导。都没有搞清楚这个错误是hiredis丢出来。</li>
<li>最后我还是觉得写入数据时候有概率写入<strong>魔鬼字符</strong>导致错误，这个需要后续跟进</li>
</ol>
<p>暂时写这么多吧。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis底层协议]]></title>
        <id>https://xiwan.github.io/post/FwWrD9HpO</id>
        <link href="https://xiwan.github.io/post/FwWrD9HpO">
        </link>
        <updated>2020-02-18T09:34:39.000Z</updated>
        <summary type="html"><![CDATA[<p>最近读到官网的一篇文章 https://redis.io/topics/protocol, 主要是描述了一下redis在通讯协议。忽然觉得豁然开朗：redis作为作为一个内存数据库，其实其本质也是一个服务器而已。监听在6379（默认）接口，然后定义了一套自己的命令方便调用者使用。</p>
<p>其实市面上的主流第三方客户端都会遵守这套协议。只不过实现的语言不一样而已。这里试着分析这套协议，并且写一个简单的“redis客户端”。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近读到官网的一篇文章 https://redis.io/topics/protocol, 主要是描述了一下redis在通讯协议。忽然觉得豁然开朗：redis作为作为一个内存数据库，其实其本质也是一个服务器而已。监听在6379（默认）接口，然后定义了一套自己的命令方便调用者使用。</p>
<p>其实市面上的主流第三方客户端都会遵守这套协议。只不过实现的语言不一样而已。这里试着分析这套协议，并且写一个简单的“redis客户端”。</p>
<!-- more -->
<h3 id="内联协议">内联协议</h3>
<p>既然是个tcp服务器，我们就可以telnet上去，类似下图这样：<br>
<img src="https://xiwan.github.io/post-images/1582019455503.png" alt=""></p>
<p>这些命令看起来和我们平时使用的差不多，不过这些命令被称为“内联命令”，而不是redis真正的通信协议。怎么理解呢？就是这些命令发过去后，redis服务器会先分析你的命令，如果发现是“内联的”，就会自己再解析成标准协议。</p>
<p>对于redis这种高性能服务器来说，花费额外的性能解析这些“内联命令”是有些得不偿失的。这里斗胆坏一下：网上有些第三方的客户端做了这种投机取巧的：）</p>
<h3 id="真正的协议">真正的协议</h3>
<p>虽然分为请求和相应两个部分，其实他们都遵循一个协议：</p>
<ul>
<li>对于<strong>简单字符串</strong>类型，它的第一个byte是&quot;+&quot;</li>
<li>对于<strong>错误</strong>类型，它的第一个byte是&quot;-&quot;</li>
<li>对于<strong>整数</strong>类型， 它的第一个byte是&quot;:&quot;</li>
<li>对于<strong>复杂字符串</strong>类型，它的第一个byte是&quot;$&quot;</li>
<li>对于<strong>数组</strong>类型，它的第一个byte是&quot;*&quot;</li>
<li><strong>结束字符</strong>固定为 &quot;\r\n&quot; (CRLF)。</li>
</ul>
<p>对于发送端：客户端永远使用<strong>数组</strong>类型，里面使用<strong>复杂字符串</strong>类型。<br>
所以举个例子，比如<em>get name</em>, 在底层就变成了</p>
<p><code>*2\r\n$3\r\nget\r\n$4\r\nname\r\n</code></p>
<p>这里做下说明： *2表示整个数组长度为2，然后\r\n都是结束字符，可以认为是redis的一种强制分隔符； $3表示get这个字符长度为3；同理, $4表示name字符长度为4。当然每个表达式中间再用CRLF字符分割即可。</p>
<p>这里我写了个简单的拼接程序：</p>
<pre><code>static readonly string CRLF = &quot;\r\n&quot;;
static string SocketCommand(string[] command) 
{
		var _len = command.Length;
		var _command = new StringBuilder();
		_command.Append($&quot;*{_len}{CRLF}&quot;);

		for (int i = 0; i &lt; _len; i++) 
		{
				_command.Append($&quot;${command[i].Length}{CRLF}{command[i]}{CRLF}&quot;);
		}

		return _command.ToString();
}
</code></pre>
<p>测试结果：</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1582021232629.png" alt=""></figure>
<p>可以看到和telnet模式是一样的操作，但底层协议完全不一样。</p>
<h3 id="不同的模式">不同的模式</h3>
<p>除了上面说得这种经典“一问一答”模式之外， redis还支持另外两种操作：</p>
<ol>
<li>管道技术(PIPELINE)： 当你有很多命令需要短时间执行时候，这个技术非常有用。其核心就是将多个命令打包成一个命令，减少来回的网络开销。这里有一点说明，管道和事务是有区别的，虽然他们都是合并命令。管道可以认为这些命令是松散的，中间可以插入其他的命令；而事务是相反的，这些命令是紧耦合的，必须全部执行完毕才能执行后续命令。</li>
<li>订阅模式：当客户端订阅(subscribe)某个频道后，redis将不再需要等待命令了。可以反过来在pub端发送消息，而sub端会自动获取到这些消息。这项技术在应用在我们游戏的聊天室中，还有一些跨服消息（rpc）之类。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random函数罢工了]]></title>
        <id>https://xiwan.github.io/post/POXwU3zqV</id>
        <link href="https://xiwan.github.io/post/POXwU3zqV">
        </link>
        <updated>2020-02-09T12:43:49.000Z</updated>
        <summary type="html"><![CDATA[<p>今天下午，突然线上出现大量高级掉落重复出现。<br>
正常的概率是1%，结果部分玩家出现了很多。导致了群里炸了锅。于是开始分析掉落部分代码：</p>
]]></summary>
        <content type="html"><![CDATA[<p>今天下午，突然线上出现大量高级掉落重复出现。<br>
正常的概率是1%，结果部分玩家出现了很多。导致了群里炸了锅。于是开始分析掉落部分代码：</p>
<!-- more -->
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1581252565576.png" alt=""></figure>
<p>这里做过一次优化，以前是不停的new 一个Random对象。后面测试时候发现效率不高，于是采用了复用的方式。现在似乎这里除了问题，分析下来应该是Random在某种条件下输出了相同值。</p>
<h3 id="问题分析">问题分析</h3>
<p>既然Random出了问题，那么有下面几个问题：<br>
1. 触发Random出问题的条件是什么？<br>
2. Random出问题后会输出什么值呢？<br>
3. 假如是固定值，是有条件固定还是无条件呢？</p>
<h4 id="触发条件">触发条件</h4>
<p>有一种说法是：由于不指定seed情况下，random会默认用时间的tick来当作种子。如果在一个tick内重复过多的调用，则会产生固定值。</p>
<p>不过根据线上情况分析：这个问题持续了半小时。所以这个马上被否决了。</p>
<p>接下来搜索后得到了一个线索：</p>
<figure data-type="image" tabindex="2"><img src="https://xiwan.github.io/post-images/1581252881498.png" alt=""></figure>
<p>并且有人提到当random停止工作后，他固定返回0。</p>
<p>多线程环境下，的确使用static的random会产生这样的问题。这样来看应该是这里对于random的优化不当了。不过随之而来的是，固定值真的是0么？</p>
<h4 id="固定值">固定值</h4>
<p>停摆后的random的固定输出值是什么？真的是0。</p>
<p>同理，线上的表现否定了这个结果。因为如果是0，玩家的掉落数目也使用了这个随机数，所以他们应该获取不到这个异常的结果。。。</p>
<p>那么究竟是什么值呢？只能通过模拟多线程来压测了</p>
<figure data-type="image" tabindex="3"><img src="https://xiwan.github.io/post-images/1581253148318.png" alt=""></figure>
<p>具体代码如上，在多线程环境下模拟随机过程，推论是：如果正常他应该输出比较均匀的分布，如果异常应该可以发现某个值特别大。</p>
<p>当次数低于100000次，我并没有看到太多异常。当超过这个值后有趣的事情发生了：</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1581253310151.png" alt=""></figure>
<figure data-type="image" tabindex="5"><img src="https://xiwan.github.io/post-images/1581253317111.png" alt=""></figure>
<figure data-type="image" tabindex="6"><img src="https://xiwan.github.io/post-images/1581253322350.png" alt=""></figure>
<p>这三次异常的结果我差不多测试了30多遍才出来。也就是说差不多只有10%的概率出现Random罢工。这样的确证实了前面的说的多线程情况下， random输出固定值。可以看到一开始应该是均匀分布的，一旦出现多线程问题，马上罢工。</p>
<h4 id="固定条件">固定条件</h4>
<p>Ranom 罢工后会输出固定值这点确认了。但不是网上大家说的是0，而是取决于我输入的min值。这样就解释了线上的大部分疑惑了。并且继续做测试可以发现，这个固定值一旦产生并不是固定不变，而是每次都等于min值。</p>
<p>测试条件为5-10，11-15两个取间同时开始随机取值，检查分布。结果如下图：</p>
<figure data-type="image" tabindex="7"><img src="https://xiwan.github.io/post-images/1581253614580.png" alt=""></figure>
<p>看到当第一个红框出现罢工后，下一个Random值不是5，而是直接都输出成11了。</p>
<h3 id="如何改进呢">如何改进呢？</h3>
<p>原来是通过new 一个Random达到隔离的目的，但现在看来有两个问题：<br>
1. 效率不高<br>
2. 并没有根本解决多线程下罢工的问题。<br>
网上给的方案是采用锁，这样感觉效率也不高。并不能给这个底层方法带来本质的变化。</p>
<p>现在想到的一个方案类似于concurrentDictionary那样，多粒度控制锁的一个Random池子。利用这个数据结构的并发性，来创造多个可以复用的Random实例。每次需要时候采用RondRobin算法获得一个对象即可。</p>
<p>当然更激进的做法就是不用并发对象，直接做1000个实例，感觉出错的概率会下降不少。<br>
以下就是核心代码部分，每次通过到队列中获取random实例，而非反复构造它。当然在竞争十分激烈的情况下有可能拿不到random，这个时候就做了个保底。使用全局的globalRandom.</p>
<pre><code>        static Random DequeueRand(Random random)
        {
            if (random == null)
            {
                if (GlobalRandomQueue.TryDequeue(out Random rand))
                {
                    random = rand;
                    GlobalRandomQueue.Enqueue(rand);
                }
                else
                {
                    // 保底
                    random = globalRandom;
                }
            }
            return random;
        }
</code></pre>
<p>测试效果：测试10批，每批10次， 每次1百万条，random函数再也不出现“罢工“情况了</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[线上redis问题分析记录]]></title>
        <id>https://xiwan.github.io/post/9Wy-WVteG</id>
        <link href="https://xiwan.github.io/post/9Wy-WVteG">
        </link>
        <updated>2020-01-17T10:08:27.000Z</updated>
        <content type="html"><![CDATA[<p>首先祭出一般的复杂操作优化方案<br>
<img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e216cc5b43ea.png" alt=""></p>
<ol>
<li>
<p>历史价格不需要拿出全部数据。看现在这部分大小不大都在1KB左右，但是由于时间长了某些物品的价格数量比较多导致list会比较长。</p>
<figure data-type="image" tabindex="1"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e216d0097915.png" alt=""></figure>
</li>
<li>
<p>部分玩家的事务队列是否可能出现堆积，然后队列过长？</p>
</li>
<li>
<p>寄售队列查了日志访问人数不多，将来可以考虑寄售过期之类，防止过长。</p>
</li>
<li>
<p>可以定一个大概取间让队列尽量控制在5000以内，排查下用队列比较多的地方，比如排行榜之类。</p>
</li>
<li>
<p>LDR:0:1 作为排行榜也是最热的key,需要控制大小和长度。</p>
<figure data-type="image" tabindex="2"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e2177ba65644.png" alt=""></figure>
</li>
<li>
<p>建议有expire的地方，如果觉得会有大量数据的前提下，都加一个随机数.像下面这种循环内设置统一过期时间的做法就有点危险</p>
</li>
</ol>
<figure data-type="image" tabindex="3"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e218b722d15b.png" alt=""></figure>
<ol start="7">
<li>
<p>保持主redis只放和用户相关的重要数据，周围数据都可以放在另外的redis中，需要梳理业务。</p>
</li>
<li>
<p>对于stackexchage的连接做保护</p>
</li>
<li>
<p>每日凌晨发现有集中的内存释放，并且空间还比较大.</p>
<figure data-type="image" tabindex="4"><img src="https://xiwan.github.io/post-images/1579255870027.png" alt=""></figure>
</li>
</ol>
<p>分析完业务后发现是排行榜集中时间过期，没有做随机延迟。<br>
排行榜大的原因是没有限制数目导致的，并且还有一个安全隐患就是设置数据和expire是两条语句，有可能导致内存泄露。</p>
<pre><code>public static async Task RenewDailyStar(RaidType type, int relatedID, RaidDailyStar star)
        {
            var oldStar = RedisValue.Null;
            var key = GetDailyStarKey(type, relatedID);

            // 防止集中过期
            var expRandom = TimeSpan.FromDays(offlineExpireDay).Add(TimeSpan.FromSeconds(MathHelper.GetRandom(0, 600))); 
            try
            {
                // fetch last daily star
                var cacheStar = SerializationHelper.Serialize(star);
                // getset is atomic
                oldStar = await RedisServer.Instance().DB(dailyStarDB).StringGetSetAsync(key, cacheStar);
                if (oldStar != RedisValue.Null)
                {
                    // compare with oldstar
                    if (oldStar.Get&lt;RaidDailyStar&gt;().GetFinalScore() &gt; star.GetFinalScore())
                        await RedisServer.Instance().DB(dailyStarDB).StringSetAsync(key, oldStar, expRandom);
                }
            }
            catch (Exception ex)
            {
                LogHelper.WriteErrorLog(string.Format(&quot;{0} | {1}&quot;, star.GetFinalScore(), ex.Message));

                // 滚回操作
                if (oldStar != RedisValue.Null)
                    await RedisServer.Instance().DB(dailyStarDB).StringSetAsync(key, oldStar, expRandom);
            }
        }
</code></pre>
<ol start="10">
<li>这里是个批处理，而不是pipeline，由于取了1000位排行榜玩家数据，所以很可能这边会相当耗时。<br>
<img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e217cfe86470.png" alt=""></li>
</ol>
<p>在最外层的调用地方还有一个for循环加持放大</p>
<p><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e21874521588.png" alt=""><br>
分析腾讯的监控看那个面板可以看到，每个一个小时延迟和读数目有个尖峰。应该就是这个原因导致的。</p>
<figure data-type="image" tabindex="5"><img src="http://192.168.1.153/showdoc/server/../Public/Uploads/2020-01-17/5e217e17cc74f.png" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[elk时区问题]]></title>
        <id>https://xiwan.github.io/post/zX5LGZjc5</id>
        <link href="https://xiwan.github.io/post/zX5LGZjc5">
        </link>
        <updated>2020-01-09T03:39:22.000Z</updated>
        <summary type="html"><![CDATA[<p>希望通过这篇文章能解释清楚elk中的时区问题</p>
]]></summary>
        <content type="html"><![CDATA[<p>希望通过这篇文章能解释清楚elk中的时区问题</p>
<!-- more -->
<p>由于kibana包括es默认是使用UTC时间，对于我们这种常年驻扎在东八的社畜来说十分不友好。比如我们日志是按照东八时间记录的，但到了elk中间你会发现@timestamp会莫名奇妙的少了8个小时。这是因为底层用的就是UTC。</p>
<p>于是找到一种方案是在Logstash中对齐@timestamp和日志时间：</p>
<pre><code>  ruby { 
    code =&gt; &quot;event['timestamp'] = LogStash::Timestamp.new(event['@timestamp']+ 8*60*60)&quot; 
  }
  
  ruby {
    code =&gt; &quot;event['@timestamp']= event['timestamp']&quot;
  }
</code></pre>
<p>这样做后的结果呢？</p>
<figure data-type="image" tabindex="1"><img src="https://xiwan.github.io/post-images/1578541551939.png" alt=""></figure>
<p>由于默认设置kibana用浏览器时间，所以档这个对齐后，上午11点却只能现实凌晨3点的日志。。。感觉越搞越糟糕了。</p>
<p>仔细想下，我们其实强行同步这两个时间是有点多余的。因为kibana默认是按照@timestamp来交割，并且用的是utc时区来展示。当我们切换设置为browser，它自动将@timestamp转换到对应时区，而不影响日志展示本身。所以一旦你强行对齐这两个时间，反而会导致部分日志无法展示出来了。如果你服务器一开始就设定UTC时间来记录日志，这种对齐是没有任何问题得。但是一旦设定了其他时区，其实最好得做法就是简单的把日志时间和时区信息设置到@timestamp，剩下的就让elk自己转换即可。</p>
<pre><code>  date {
    match =&gt; [&quot;[log_json][log_datetime]&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;]
    target =&gt; &quot;@timestamp&quot;
    timezone =&gt; &quot;+08:00&quot;
  }
</code></pre>
<p>但是要记得在查询语句里面和时间相关的条件都带上timezone就好了。其实这么看来，最优雅的做法还是一开始就把服务器时间设置成UTC,可以少走很多弯路。</p>
<p>这里多说一点，elk用这个@timestamp除了展示用还有什么作用呢？索引分割用。由于我们日志是按天分割，日志文件是按小时记录。由于存在8小时时差，所以索引的一天24小时是跨了日志的两天的。比如索引7号的日志，实际是从7号早上8点到8号早上7点为止。这点明白后，在调整索引时候，最好延迟两天再做，防止对正在写入的索引产生影响。别我是怎么知道的，反正都是血泪史。</p>
]]></content>
    </entry>
</feed>